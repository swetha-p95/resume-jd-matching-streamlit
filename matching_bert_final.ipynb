{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['SQL', 'Excel', 'NOSQL', 'Python']\n"
          ]
        }
      ],
      "source": [
        "#resume skill ner extraction\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import pprint\n",
        "\n",
        "nlp = spacy.load(\"C:\\\\Users\\\\HP\\\\Desktop\\\\DUK\\\\S2\\\\NLP\\\\project_final\\\\new_model_resume\")\n",
        "\n",
        "doc = nlp(\"Harini Komaravelli Test Analyst at Oracle, Hyderabad  Hyderabad, Telangana - Email me on Indeed: indeed.com/r/Harini- Komaravelli/2659eee82e435d1b  Hartej Kathuria Data Analyst Intern - Oracle Retail  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Hartej-Kathuria/04181c5962a4af19  Willing to relocate to: Delhi - Bangalore, Karnataka - Gurgaon, Haryana  WORK EXPERIENCE  Data Analyst Intern  Oracle Retail -  Bengaluru, Karnataka -  June 2017 to Present  Job Responsibilities: o As an intern part of the Global Retail Insights team at Oracle Retail, work involved creating a data oriented buisness case based using high level trends for various retailers using Excel and SQL. o Forecasting Sales with use of various statistical Modelling Methods using SQL and R o Market Basket Analysis using transactional data of retailers using SQL and R  EDUCATION  Statistics and Probability  Manipal University  May 2018  B. Tech in Electrical and Electronics in Embedded Systems  MIT, Manipal University  May 2016  SKILLS  Python (2 years), SQL. (1 year), NOSQL (1 year), R (2 years), Machine Learning (2 years)  PUBLICATIONS  Post-operative life expectancy in lung cancer patients  The objective of the project was to build an efficient predictive model based on a predefined dataset to predict whether the patient survives or dies within one year of the operation. The dataset given has 17 variables: 12 nominal, 2 ordinal and 3 numerical. The target variable has value true if the patient dies within one year of the operation else false if he survives. Tool used: R  https://www.indeed.com/r/Hartej-Kathuria/04181c5962a4af19?isid=rex-download&ikw=download-top&co=IN   Predict the Happiness (Sentimental Analysis)  The objective of this project was to build a binary classifcation model for the data provided by TripAdvisor consisiting of a sample of hotel reviews provided by customers.The model built can be used by them to understand the hotels listed by them.Tool Used: R  Predict Network attacks  The objective of this project was to build a multi-class classification model to predict the type of attack for an internet network company in Japan which has been facing huge losses due to malicious server attacks.The train dataset has 18 numerical features and 23 categorical features.The target variable has three classes.Tool Used: Python  ADDITIONAL INFORMATION  TECHNICAL SKILLSET  • Languages & Technologies: Python, R, SQL, NoSQL, Predictive Modelling, Market Basket Analysis, Sentimental Analysis, Clustering, Bash Scripting (Preliminary), Socket Programming, Java (Preliminary)  • Tools: R Studio, Jupyter, GIT, Sublime, MATLAB, Linux, KVM, Virtual Box, Open VZ, Oracle SQL Developer, MySQL, MongoDB, Excel\")\n",
        "\n",
        "\n",
        "entities = {label: [] for label in nlp.get_pipe(\"ner\").labels}\n",
        "for ent in doc.ents:\n",
        "        if ent.label_ in nlp.get_pipe(\"ner\").labels:\n",
        "            entities[ent.label_].append(ent.text)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "token_list = []\n",
        "for item in entities[\"SKILL\"]:\n",
        "    text = item\n",
        "    tokens = word_tokenize(text)\n",
        "    token_list = token_list + tokens\n",
        "\n",
        "token_list_resume_correct=[]\n",
        "for i in token_list:\n",
        "     if i.isalpha():\n",
        "          token_list_resume_correct.append(i)\n",
        "token_list_correct = list(set(token_list_resume_correct))\n",
        "print(token_list_correct)        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Required', 'GCP', 'skills', 'Excel', 'entry', 'PowerPoint']\n"
          ]
        }
      ],
      "source": [
        "#jd skill ner extraction\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import pprint\n",
        "\n",
        "nlp = spacy.load(\"new_model_jd\")\n",
        "\n",
        "doc = nlp(\"RESPONSIBILITIES Complete appropriate role-specific training to perform job duties Under supervision, perform assigned administrative tasks to support team members with project execution examples of such tasks include but not limited to running system reports, maintaining minutes of meeting, preparing and distributing status reports, creating and maintaining study documents, etc. Under supervision, assist in updating and maintaining systems within project timelines and per project plans Maintaining the source documents, CRF’s, Investigator Site File and other study related documents according to ICH-GCP guideline Completing the eCRF Maintenance of Investigational Product according to the specific temperature. Preparing for site qualification visits, initiation visits, monitoring visits and site close out visits. Organizing ethics committee meetings Completing the review forms and submitting all study related documents to EC Coordinating with the local labs, central labs Assistance in Safety Reporting within the required timelines. Assistance in patient selection and recruitment Assistance in Informed Consent Process REQUIRED KNOWLEDGE, SKILLS AND ABILITIES Basic knowledge of applicable research and regulatory requirements, i.e., ICH GCP and relevant local laws, regulations and guidelines Exposure to MS Applications including but not limited to Microsoft Word, Excel and PowerPoint Effective written and verbal communication skills including good command of English language Effective time management skillsResults and detail-oriented approach to work delivery and output Ability to establish and maintain effective working relationships with coworkers, managers and clients Strong listening and phone skills Good data entry skills Required Experience, Skills and Qualifications MSc Biotechnology, Microbiology, Biochemistry B. Pharm, M. Pharm, Pharm D. MSc. Bsc. LifeScience: Bioinformatics, Biomedical Engineering ect.Job Types: Full-time, Internship\")\n",
        "# doc = nlp(\"opportunity work worldfamous federally funded research development organization great culture need Software Engineer provide support software team implementing integrating pythonbased analysis capabilities resiliency testbed along angular web front ends Key Points Title Software Engineer Location Lexington Greater Boston Job Type W2 Contract Position ScopeJob Functions Develop Pythonbased analysis capabilities resiliency testbed includes ingesting processing experimental data Testbed producing value insights using statistical MLbased techniques Develop Angular web front ends Develop web app front ends Angular communicates various backend technologies APIs Infrastructure Automation Docker Ansible Terraform includes developing automation configuration deployment software services AWS ESX environments using containers Ansible including infrastructure configuration Terraform task also includes using automation secure harden deployed services Javabased applications data processing adaptation Develop Java applications primarily focused ingesting adapting normalizing data downstream analytics Secondary tasks include developing Springbootbased Java microservices Also perform related testing compose compile maintain documentation users administrators secondary role assist development team planning execution effective agile software development practices Must able communicate collaborate work effectively within RST team software developer must willingness take new technologies work areas take responsibilities necessarily expert Skills Required Experience developing nontrivial applications using Python Java Angular JSON Protobufs Maven Agile software development source code version control systems ie git Github Experience developing deployment pipelines using Docker Ansible Terraform Experience securing hardening deployed services using Ansible Experience Elastic Stack Apache NIFI Unit testing integration testing Developing debugging executing software Windows Linux environment Producing editing maintaining documentation aimed users developers Experience ActiveMQ Kafka middleware messaging platforms Education Bachelor s degree 5 years relevant work experience Work Authorization US Citizenship required\")\n",
        "# doc = nlp(\"Infosys is seeking Machine learning Engineer. This position will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition, and Design; play an important role in creating the high-level design artifacts; deliver high-quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty; be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and value Required Qualifications: Candidate must be located within commuting distance of Tampa, FL, or be willing to relocate to the area. This position may require travel in the US. Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. At least 4 years of Information Technology experience U.S. citizens and those authorized to work in the U.S. are encouraged to apply, we are unable to sponsor at this time. Preferred Qualifications: Experience with research and development of ML-based solutions, including the productization, deployment, and lifecycle management of production-level ML models. Pipelines Experience in developing ML solutions with Python ML Stack (Pandas, Matplotlib, SciKitLearn) , Jupyter, Keras/tensor flow Experience with Big Data processing: Hadoop, Redis, Spark, Big Query, Data Lake, Qlik View Experience working on one of DB technologies, writing queries etc. – Oracle, No-SQL, PL/SQL or Postgres Work with large complex data sets to extract analyze, visualize and infer meaningful insights Design and monitor tools to measure a particular problem or the contribution of a particular technique over time Work with product management and application development teams to identify, prototype, develop and deploy ML models. Experience developing cloud-based applications, preferably AWS, GCC Experience of full-stack development of enterprise applications using Java/J2EE, JavaScript, SOAP, REST Webservices, Spring Boot, Microservices\")\n",
        "entities = {label: [] for label in nlp.get_pipe(\"ner\").labels}\n",
        "for ent in doc.ents:\n",
        "        if ent.label_ in nlp.get_pipe(\"ner\").labels:\n",
        "            entities[ent.label_].append(ent.text)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "token_list_jd = []\n",
        "for item in entities[\"SKILL\"]:\n",
        "    text = item\n",
        "    tokens = word_tokenize(text)\n",
        "    token_list_jd = token_list_jd + tokens\n",
        "\n",
        "token_list_jd_correct = []\n",
        "for i in token_list_jd:\n",
        "     if i.isalpha():\n",
        "          token_list_jd_correct.append(i)\n",
        "token_list_jd_correct = list(set(token_list_jd_correct))\n",
        "print(token_list_jd_correct)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For BERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "## Want DistilBERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.484, 3, 1)\n"
          ]
        }
      ],
      "source": [
        "def get_sw_embeddings(word):\n",
        "   \n",
        "    # Tokenize the word\n",
        "    encoded_input = tokenizer(word, return_tensors='pt')\n",
        "    \n",
        "    # Get the token IDs and attention mask\n",
        "    input_ids = encoded_input['input_ids']\n",
        "    attention_mask = encoded_input['attention_mask']\n",
        "    \n",
        "    # Get the embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "    \n",
        "    # Remove the [CLS] and [SEP] tokens\n",
        "    token_embeddings = last_hidden_states[0][1:-1]\n",
        "    \n",
        "    # If the word was split into subwords, average their embeddings\n",
        "    word_embedding = token_embeddings.mean(dim=0)\n",
        "    \n",
        "    return word_embedding.numpy()\n",
        "\n",
        "\n",
        "def sw_semantic_similarity_from_bert(job,resume):\n",
        "    \"\"\"calculate similarity with bertbaseuncased\"\"\"\n",
        "    score = []\n",
        "    match_count = 0\n",
        "    sim_count = 0\n",
        "    for i in job:\n",
        "        sim_score = []\n",
        "        for j in resume:\n",
        "            job_emb = get_sw_embeddings(i).reshape(1,-1)\n",
        "            resume_emb = get_sw_embeddings(j).reshape(1,-1)\n",
        "            sim_score.append(cosine_similarity(job_emb,resume_emb))\n",
        "        if np.array(sim_score).max()>0.6:\n",
        "          score.append(np.array(sim_score).max())\n",
        "          if  np.array(sim_score).max() == 1:\n",
        "            match_count+=1 \n",
        "          else:\n",
        "            sim_count += 1\n",
        "                 \n",
        "\n",
        "    score_sum = np.array(score).sum()/len(job)\n",
        "\n",
        "   \n",
        "    return round(score_sum,3), sim_count, match_count\n",
        "\n",
        "score = sw_semantic_similarity_from_bert(token_list_jd_correct,token_list_resume_correct)\n",
        "print(score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
