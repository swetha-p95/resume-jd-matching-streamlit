{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e744e7-c088-434d-a7d1-85213c3e7c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c94d8d-115b-4996-a09f-5995ded62075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "#print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b4b020-f3cc-47ce-bd30-9d89e69b8058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.KeyedVectors"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = api.load('word2vec-google-news-300')\n",
    "# wv = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "type(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6845a37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOSQL', 'Python', 'SQL', 'Excel']\n"
     ]
    }
   ],
   "source": [
    "#resume skill extraction\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pprint\n",
    "\n",
    "nlp = spacy.load(\"new_model_resume\")\n",
    "\n",
    "doc = nlp(\"Harini Komaravelli Test Analyst at Oracle, Hyderabad  Hyderabad, Telangana - Email me on Indeed: indeed.com/r/Harini- Komaravelli/2659eee82e435d1b  Hartej Kathuria Data Analyst Intern - Oracle Retail  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Hartej-Kathuria/04181c5962a4af19  Willing to relocate to: Delhi - Bangalore, Karnataka - Gurgaon, Haryana  WORK EXPERIENCE  Data Analyst Intern  Oracle Retail -  Bengaluru, Karnataka -  June 2017 to Present  Job Responsibilities: o As an intern part of the Global Retail Insights team at Oracle Retail, work involved creating a data oriented buisness case based using high level trends for various retailers using Excel and SQL. o Forecasting Sales with use of various statistical Modelling Methods using SQL and R o Market Basket Analysis using transactional data of retailers using SQL and R  EDUCATION  Statistics and Probability  Manipal University  May 2018  B. Tech in Electrical and Electronics in Embedded Systems  MIT, Manipal University  May 2016  SKILLS  Python (2 years), SQL. (1 year), NOSQL (1 year), R (2 years), Machine Learning (2 years)  PUBLICATIONS  Post-operative life expectancy in lung cancer patients  The objective of the project was to build an efficient predictive model based on a predefined dataset to predict whether the patient survives or dies within one year of the operation. The dataset given has 17 variables: 12 nominal, 2 ordinal and 3 numerical. The target variable has value true if the patient dies within one year of the operation else false if he survives. Tool used: R  https://www.indeed.com/r/Hartej-Kathuria/04181c5962a4af19?isid=rex-download&ikw=download-top&co=IN   Predict the Happiness (Sentimental Analysis)  The objective of this project was to build a binary classifcation model for the data provided by TripAdvisor consisiting of a sample of hotel reviews provided by customers.The model built can be used by them to understand the hotels listed by them.Tool Used: R  Predict Network attacks  The objective of this project was to build a multi-class classification model to predict the type of attack for an internet network company in Japan which has been facing huge losses due to malicious server attacks.The train dataset has 18 numerical features and 23 categorical features.The target variable has three classes.Tool Used: Python  ADDITIONAL INFORMATION  TECHNICAL SKILLSET  • Languages & Technologies: Python, R, SQL, NoSQL, Predictive Modelling, Market Basket Analysis, Sentimental Analysis, Clustering, Bash Scripting (Preliminary), Socket Programming, Java (Preliminary)  • Tools: R Studio, Jupyter, GIT, Sublime, MATLAB, Linux, KVM, Virtual Box, Open VZ, Oracle SQL Developer, MySQL, MongoDB, Excel\")\n",
    "\n",
    "entities = {label: [] for label in nlp.get_pipe(\"ner\").labels}\n",
    "for ent in doc.ents:\n",
    "        if ent.label_ in nlp.get_pipe(\"ner\").labels:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "token_list_resume = []\n",
    "for item in entities[\"SKILL\"]:\n",
    "    text = item\n",
    "    tokens = word_tokenize(text)\n",
    "    token_list_resume = token_list_resume+ tokens\n",
    "\n",
    "token_list_resume_correct=[]\n",
    "for i in token_list_resume:\n",
    "     if i.isalpha():\n",
    "          token_list_resume_correct.append(i)\n",
    "token_list_resume_correct = list(set(token_list_resume_correct))\n",
    "print(token_list_resume_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "733a3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GCP', 'Required', 'entry', 'PowerPoint', 'Excel', 'skills']\n"
     ]
    }
   ],
   "source": [
    "#jd skill extraction\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import pprint\n",
    "\n",
    "nlp = spacy.load(\"new_model_jd\")\n",
    "\n",
    "doc = nlp(\"RESPONSIBILITIES Complete appropriate role-specific training to perform job duties Under supervision, perform assigned administrative tasks to support team members with project execution examples of such tasks include but not limited to running system reports, maintaining minutes of meeting, preparing and distributing status reports, creating and maintaining study documents, etc. Under supervision, assist in updating and maintaining systems within project timelines and per project plans Maintaining the source documents, CRF’s, Investigator Site File and other study related documents according to ICH-GCP guideline Completing the eCRF Maintenance of Investigational Product according to the specific temperature. Preparing for site qualification visits, initiation visits, monitoring visits and site close out visits. Organizing ethics committee meetings Completing the review forms and submitting all study related documents to EC Coordinating with the local labs, central labs Assistance in Safety Reporting within the required timelines. Assistance in patient selection and recruitment Assistance in Informed Consent Process REQUIRED KNOWLEDGE, SKILLS AND ABILITIES Basic knowledge of applicable research and regulatory requirements, i.e., ICH GCP and relevant local laws, regulations and guidelines Exposure to MS Applications including but not limited to Microsoft Word, Excel and PowerPoint Effective written and verbal communication skills including good command of English language Effective time management skillsResults and detail-oriented approach to work delivery and output Ability to establish and maintain effective working relationships with coworkers, managers and clients Strong listening and phone skills Good data entry skills Required Experience, Skills and Qualifications MSc Biotechnology, Microbiology, Biochemistry B. Pharm, M. Pharm, Pharm D. MSc. Bsc. LifeScience: Bioinformatics, Biomedical Engineering ect.Job Types: Full-time, Internship\")\n",
    "# doc = nlp(\"Infosys is seeking Machine learning Engineer. This position will interface with key stakeholders and apply your technical proficiency across different stages of the Software Development Life Cycle including Requirements Elicitation, Application Architecture definition, and Design; play an important role in creating the high-level design artifacts; deliver high-quality code deliverables for a module, lead validation for all types of testing and support activities related to implementation, transition and warranty; be part of a learning culture, where teamwork and collaboration are encouraged, excellence is rewarded, and diversity is respected and value Required Qualifications: Candidate must be located within commuting distance of Tampa, FL, or be willing to relocate to the area. This position may require travel in the US. Bachelor’s degree or foreign equivalent required from an accredited institution. Will also consider three years of progressive experience in the specialty in lieu of every year of education. At least 4 years of Information Technology experience U.S. citizens and those authorized to work in the U.S. are encouraged to apply, we are unable to sponsor at this time. Preferred Qualifications: Experience with research and development of ML-based solutions, including the productization, deployment, and lifecycle management of production-level ML models. Pipelines Experience in developing ML solutions with Python ML Stack (Pandas, Matplotlib, SciKitLearn) , Jupyter, Keras/tensor flow Experience with Big Data processing: Hadoop, Redis, Spark, Big Query, Data Lake, Qlik View Experience working on one of DB technologies, writing queries etc. – Oracle, No-SQL, PL/SQL or Postgres Work with large complex data sets to extract analyze, visualize and infer meaningful insights Design and monitor tools to measure a particular problem or the contribution of a particular technique over time Work with product management and application development teams to identify, prototype, develop and deploy ML models. Experience developing cloud-based applications, preferably AWS, GCC Experience of full-stack development of enterprise applications using Java/J2EE, JavaScript, SOAP, REST Webservices, Spring Boot, Microservices\")\n",
    "entities = {label: [] for label in nlp.get_pipe(\"ner\").labels}\n",
    "for ent in doc.ents:\n",
    "        if ent.label_ in nlp.get_pipe(\"ner\").labels:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "\n",
    "# print(entities[\"SKILL\"])\n",
    "from nltk.tokenize import word_tokenize\n",
    "token_list_jd = []\n",
    "for item in entities[\"SKILL\"]:\n",
    "    text = item\n",
    "    tokens = word_tokenize(text)\n",
    "    token_list_jd = token_list_jd + tokens\n",
    "\n",
    "token_list_jd_correct = []\n",
    "for i in token_list_jd:\n",
    "     if i.isalpha():\n",
    "          token_list_jd_correct.append(i)\n",
    "token_list_jd_correct = list(set(token_list_jd_correct))\n",
    "print(token_list_jd_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d63c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.167, 0, 5)\n"
     ]
    }
   ],
   "source": [
    "def semantic_similarity_word2vec(job,resume):\n",
    "    \"\"\"calculate similarity with word2vec\"\"\"\n",
    "    \n",
    "    score = 0\n",
    "    oov = 0\n",
    "    sim_count = 0\n",
    "        \n",
    "    for i in range(len(job)):\n",
    "        \n",
    "        if job[i] in resume:\n",
    "            score += 1\n",
    "            \n",
    "        else:\n",
    "          sim_score = np.zeros(len(resume))\n",
    "        #   print(sim_score)\n",
    "          for j in range(len(resume)):\n",
    "              try:\n",
    "               sim_score[j] = wv.similarity(job[i],resume[j])\n",
    "              except KeyError:\n",
    "                 oov += 1\n",
    "        #   print(sim_score)\n",
    "          if sim_score.max() > 0.6:\n",
    "            score += sim_score.max()\n",
    "            sim_count += 1\n",
    "            # print(sim_score.max())\n",
    "        # print(score)\n",
    "    score = score/len(job)  \n",
    "    return round(score,3),sim_count,oov\n",
    "\n",
    "print(semantic_similarity_word2vec(token_list_jd_correct,token_list_resume_correct))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
